{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **GPT & LangChain**"
      ],
      "metadata": {
        "id": "vo_mVPXlBi-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "7gpolRICBmNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai langchain tiktoken pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adjgUeU__Co9",
        "outputId": "82bb6477-6b26-4fe3-bf07-de89fd182973"
      },
      "execution_count": 292,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.287)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.5.1)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (3.16.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.20)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.5.14)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.36)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.5)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.12)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "import datetime\n",
        "import json\n",
        "from IPython.display import display, Markdown\n"
      ],
      "metadata": {
        "id": "vmpEMH1lEZbg"
      },
      "execution_count": 293,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# API Key setup"
      ],
      "metadata": {
        "id": "fKZRqaWuBoKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# from dotenv import load_dotenv, find_dotenv\n",
        "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
        "os.environ['OPENAI_API_KEY'] = \"your-key\"\n",
        "\n",
        "openai.api_key = os.environ['OPENAI_API_KEY']\n",
        "\n",
        "current_date = datetime.datetime.now().date()\n",
        "if current_date < datetime.date(2025, 9, 2):\n",
        "    llm_name = \"gpt-3.5-turbo-16k-0613\"\n",
        "else:\n",
        "    llm_name = \"gpt-3.5-turbo\"\n",
        "print(f\"llm name: {llm_name}\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQyUOYX8-r_8",
        "outputId": "91c9d719-d3d4-4d3c-8d9c-50f80051a2a7"
      },
      "execution_count": 295,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llm name: gpt-3.5-turbo-16k-0613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Базовий запит в GPT model"
      ],
      "metadata": {
        "id": "TnAA4hWdBqr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0,\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]\n"
      ],
      "metadata": {
        "id": "MG7DVMoW-sLT"
      },
      "execution_count": 296,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://platform.openai.com/docs/api-reference/completions/create"
      ],
      "metadata": {
        "id": "H1GVGLiXzzO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Create a comprehensive step-by-step guide in Ukrainian with code snippets for a newbie on how to use openai ChatCompletion.\n",
        "\"\"\"\n",
        "\n",
        "response = get_completion(prompt)\n",
        "Markdown(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 931
        },
        "id": "V6Gnfr1J9zdp",
        "outputId": "8db05c61-e85c-4495-b91f-64a1c31ae803"
      },
      "execution_count": 297,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Крок 1: Установка OpenAI Python SDK\n\nПерш за все, вам потрібно встановити OpenAI Python SDK на своєму комп'ютері. Ви можете зробити це, використовуючи pip, командою:\n\n```\npip install openai\n```\n\nКрок 2: Отримання API ключа\n\nДля використання OpenAI ChatCompletion вам потрібен API ключ. Ви можете отримати його, створивши обліковий запис на веб-сайті OpenAI. Після цього ви отримаєте ключ, який потрібно буде використовувати для аутентифікації.\n\nКрок 3: Імпорт бібліотеки та налаштування API ключа\n\nІмпортуйте бібліотеку OpenAI та встановіть API ключ, використовуючи наступний код:\n\n```python\nimport openai\n\nopenai.api_key = 'YOUR_API_KEY'\n```\n\nЗамість `YOUR_API_KEY` вкажіть свій отриманий API ключ.\n\nКрок 4: Використання OpenAI ChatCompletion\n\nТепер ви готові використовувати OpenAI ChatCompletion для отримання відповідей на ваші запитання. Використовуйте наступний код:\n\n```python\nresponse = openai.Completion.create(\n  engine=\"davinci-codex\",\n  prompt=\"Запитання: Які є найпопулярніші мови програмування?\",\n  max_tokens=100,\n  temperature=0.7,\n  n=1,\n  stop=None,\n  log_level=\"info\"\n)\n\nanswer = response.choices[0].text.strip()\nprint(answer)\n```\n\nУ цьому коді ми використовуємо модель `davinci-codex`, яка є однією з моделей OpenAI ChatCompletion. Ви можете змінити модель на іншу, якщо потрібно.\n\n`prompt` - це ваше запитання або контекст, на основі якого ви хочете отримати відповідь.\n\n`max_tokens` - це максимальна кількість токенів у відповіді. Ви можете змінити це значення відповідно до своїх потреб.\n\n`temperature` - це параметр, який впливає на різноманітність відповідей. Значення 0.7 зазвичай працює добре, але ви можете змінити його за бажанням.\n\n`n` - це кількість варіантів відповідей, які ви хочете отримати.\n\n`stop` - це список слів або фраз, які вказують моделі зупинити генерацію тексту.\n\n`log_level` - це рівень журналювання, який ви хочете використовувати.\n\nКрок 5: Обробка відповіді\n\nОтриману відповідь можна обробити за допомогою різних методів, таких як очищення від зайвих символів або видалення непотрібних розділових знаків. Наприклад, ви можете використати наступний код для очищення відповіді:\n\n```python\nimport re\n\ncleaned_answer = re.sub(r'[^\\w\\s]', '', answer)\nprint(cleaned_answer)\n```\n\nЦей код використовує регулярні вирази для видалення всіх символів, крім букв і пробілів.\n\nЦе все! Ви успішно створили початковий керівництво з використання OpenAI ChatCompletion. За допомогою цих кроків ви зможете отримувати відповіді на свої запитання з використанням OpenAI ChatCompletion."
          },
          "metadata": {},
          "execution_count": 297
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prompt Template**"
      ],
      "metadata": {
        "id": "GO_R7q2rB2LJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt = \"\"\"\n",
        "Create a comprehensive step-by-step guide in {language} with code snippets for a {specialist_level} user on {topic}.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "ZYkAwlv7APw-"
      },
      "execution_count": 298,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(prompt)\n",
        "prompt_template\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoeHyQUS_dhC",
        "outputId": "b0d8ad1f-43cf-436a-e08e-a4caf15f41cb"
      },
      "execution_count": 299,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['language', 'specialist_level', 'topic'], output_parser=None, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language', 'specialist_level', 'topic'], output_parser=None, partial_variables={}, template='\\nCreate a comprehensive step-by-step guide in {language} with code snippets for a {specialist_level} user on {topic}.\\n', template_format='f-string', validate_template=True), additional_kwargs={})])"
            ]
          },
          "metadata": {},
          "execution_count": 299
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "language = \"UA\"\n",
        "specialist_level = \"Beginner\"\n",
        "topic = \"how to use openai library\"\n",
        "\n",
        "customer_messages = prompt_template.format_messages(language=language,\n",
        "                                                    specialist_level=specialist_level,\n",
        "                                                    topic=topic\n",
        "                                                    )\n",
        "customer_messages\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qZRimOZ_dmB",
        "outputId": "5a6defc5-cc81-4897-bfd9-dd094c2a9f4e"
      },
      "execution_count": 300,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='\\nCreate a comprehensive step-by-step guide in UA with code snippets for a Beginner user on how to use openai library.\\n', additional_kwargs={}, example=False)]"
            ]
          },
          "metadata": {},
          "execution_count": 300
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# To control the randomness and creativity of the generated\n",
        "# text by an LLM, use temperature = 0.0\n",
        "chat = ChatOpenAI(temperature=0.0)\n",
        "chat\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIO5sW3bBXDQ",
        "outputId": "5159da11-582c-48d0-ecda-f9d0845af09a"
      },
      "execution_count": 301,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None)"
            ]
          },
          "metadata": {},
          "execution_count": 301
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customer_response = chat(customer_messages)\n",
        "Markdown(customer_response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "x0Z_ZqJA_drF",
        "outputId": "0defb86f-fccf-4a10-b1fa-d91c6d84c0fd"
      },
      "execution_count": 302,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Крок за кроком посібник для початківця з використання бібліотеки OpenAI:\n\nКрок 1: Встановлення бібліотеки OpenAI\n   Відкрийте командний рядок або термінал та введіть наступну команду для встановлення бібліотеки OpenAI:\n   ```\n   pip install openai\n   ```\n\nКрок 2: Отримання API ключа OpenAI\n   Для використання бібліотеки OpenAI вам потрібен API ключ. Щоб отримати його, перейдіть на веб-сайт OpenAI та створіть обліковий запис. Після цього ви отримаєте свій API ключ.\n\nКрок 3: Імпорт бібліотеки та налаштування API ключа\n   Відкрийте свій редактор коду та створіть новий файл Python. Імпортуйте бібліотеку OpenAI та встановіть свій API ключ:\n   ```python\n   import openai\n\n   openai.api_key = 'YOUR_API_KEY'\n   ```\n\nКрок 4: Використання моделей OpenAI\n   OpenAI надає різні моделі для вирішення різних задач. Одна з них - GPT-3, яка може генерувати текст на основі вхідних даних. Ось приклад використання моделі GPT-3 для генерації тексту:\n   ```python\n   response = openai.Completion.create(\n       engine='davinci',\n       prompt='Once upon a time',\n       max_tokens=100\n   )\n\n   generated_text = response.choices[0].text\n   print(generated_text)\n   ```\n\nКрок 5: Використання інших функцій OpenAI\n   OpenAI також надає інші функції, такі як переклад тексту, аналіз тональності та багато іншого. Ось приклад використання функції перекладу тексту:\n   ```python\n   response = openai.Translation.create(\n       engine='davinci',\n       text='Hello, how are you?',\n       target_language='fr'\n   )\n\n   translated_text = response.translations[0].text\n   print(translated_text)\n   ```\n\nЦе загальний посібник для початківця з використання бібліотеки OpenAI. З цими кроками ви зможете почати використовувати OpenAI для різних завдань. Не забудьте звернутися до документації OpenAI для отримання додаткової інформації та прикладів використання."
          },
          "metadata": {},
          "execution_count": 302
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Output Parser**"
      ],
      "metadata": {
        "id": "iS4-aUX_CAA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import ResponseSchema\n",
        "from langchain.output_parsers import StructuredOutputParser"
      ],
      "metadata": {
        "id": "eaMwN53y_dve"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Schema for 'Guide'\n",
        "guide_schema = ResponseSchema(name=\"Guide\",\n",
        "                              description=\"\"\"Create a comprehensive step-by-step guide integrating into 1 key 'Guide'\"\"\")\n",
        "\n",
        "# guide_schema = ResponseSchema(name=\"Guide\",\n",
        "#                               description=\"\"\"Create a comprehensive step-by-step guide integrating into 1 key 'Guide'\"\"\")\n",
        "\n",
        "# guide_schema = ResponseSchema(name=\"Guide\",\n",
        "#                               description=\"\"\"Create a comprehensive step-by-step guide integrating into 1 key 'Guide'\"\"\")\n",
        "\n",
        "response_schemas = [\n",
        "    guide_schema#, schema2, schema3\n",
        "    ]\n",
        "\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "print(format_instructions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHY6IsCK_d0E",
        "outputId": "ea92ba2c-0b5c-4d43-b110-a6ea8e3bc2c0"
      },
      "execution_count": 303,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"Guide\": string  // Create a comprehensive step-by-step guide integrating into 1 key 'Guide'\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt = \"\"\"\n",
        "Follow Instruction:\n",
        "1. Create a comprehensive step-by-step guide in the {language} language with code snippets for a {specialist_level} user on {topic}.\n",
        "2. {format_instructions}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(prompt)\n",
        "\n",
        "\n",
        "language = \"Ukrainian\"\n",
        "specialist_level = \"Beginner\"\n",
        "topic = \"how to use openai library\"\n",
        "\n",
        "customer_messages = prompt_template.format_messages(language=language,\n",
        "                                                    specialist_level=specialist_level,\n",
        "                                                    topic=topic,\n",
        "                                                    format_instructions=format_instructions\n",
        "                                                    )\n",
        "customer_response = chat(customer_messages)\n",
        "Markdown(customer_response.content)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "Q8Z6BYch_d4o",
        "outputId": "f831d40e-115f-4026-a003-9a69625014e7"
      },
      "execution_count": 308,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```json\n{\n\t\"Guide\": \"Крок за кроком посібник для початківців з використання бібліотеки OpenAI\",\n\t\"Крок 1\": \"Встановіть бібліотеку OpenAI, використовуючи pip команду:\",\n\t\"Код\": \"pip install openai\",\n\t\"Крок 2\": \"Імпортуйте бібліотеку OpenAI у свій проект:\",\n\t\"Код\": \"import openai\",\n\t\"Крок 3\": \"Отримайте API ключ для використання OpenAI. Ви можете зареєструватися на веб-сайті OpenAI та отримати ключ API.\",\n\t\"Крок 4\": \"Встановіть ваш API ключ, використовуючи наступний код:\",\n\t\"Код\": \"openai.api_key = 'Ваш_ключ_API'\",\n\t\"Крок 5\": \"Використовуйте функції OpenAI для виконання різних завдань. Наприклад, для генерації тексту використовуйте функцію 'openai.Completion.create()':\",\n\t\"Код\": \"response = openai.Completion.create(\\n    engine='davinci',\\n    prompt='Привіт, як справи?',\\n    max_tokens=50\\n)\",\n\t\"Крок 6\": \"Отримайте результат виконання запиту, використовуючи властивість 'choices' об'єкта 'response':\",\n\t\"Код\": \"result = response['choices'][0]['text']\",\n\t\"Крок 7\": \"Виведіть результат:\",\n\t\"Код\": \"print(result)\"\n}\n```"
          },
          "metadata": {},
          "execution_count": 308
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dict = output_parser.parse(customer_response.content)\n",
        "output_dict\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WKFhoaeELzW",
        "outputId": "23aff307-e6f5-47b5-e8cc-85ef31d8457d"
      },
      "execution_count": 309,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Guide': 'Крок за кроком посібник для початківців з використання бібліотеки OpenAI',\n",
              " 'Крок 1': 'Встановіть бібліотеку OpenAI, використовуючи pip команду:',\n",
              " 'Код': 'print(result)',\n",
              " 'Крок 2': 'Імпортуйте бібліотеку OpenAI у свій проект:',\n",
              " 'Крок 3': 'Отримайте API ключ для використання OpenAI. Ви можете зареєструватися на веб-сайті OpenAI та отримати ключ API.',\n",
              " 'Крок 4': 'Встановіть ваш API ключ, використовуючи наступний код:',\n",
              " 'Крок 5': \"Використовуйте функції OpenAI для виконання різних завдань. Наприклад, для генерації тексту використовуйте функцію 'openai.Completion.create()':\",\n",
              " 'Крок 6': \"Отримайте результат виконання запиту, використовуючи властивість 'choices' об'єкта 'response':\",\n",
              " 'Крок 7': 'Виведіть результат:'}"
            ]
          },
          "metadata": {},
          "execution_count": 309
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dict.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0RHKI4zEg1_",
        "outputId": "4c07a486-5b39-42f7-9605-4537f86bc04c"
      },
      "execution_count": 310,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['Guide', 'Крок 1', 'Код', 'Крок 2', 'Крок 3', 'Крок 4', 'Крок 5', 'Крок 6', 'Крок 7'])"
            ]
          },
          "metadata": {},
          "execution_count": 310
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(output_dict[\"Guide\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "O6dCJUem_eKN",
        "outputId": "7efe0186-190b-48fd-86f8-a3c0ea142d00"
      },
      "execution_count": 311,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Крок за кроком посібник для початківців з використання бібліотеки OpenAI"
          },
          "metadata": {},
          "execution_count": 311
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Memory**"
      ],
      "metadata": {
        "id": "hNoFxmOfI-aa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n"
      ],
      "metadata": {
        "id": "V-pEBCQ8GFvX"
      },
      "execution_count": 312,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ConversationBufferMemory**"
      ],
      "metadata": {
        "id": "fg8J7vrvNPhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0.0)\n",
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "GFt_2AySGFg5"
      },
      "execution_count": 313,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Привіт, мене звати Андрій\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "c4db0wy1GEo-",
        "outputId": "677fadc3-1a36-411a-94aa-3f663127e7ce"
      },
      "execution_count": 315,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Andrew\n",
            "AI: Hello Andrew! It's nice to meet you. How can I assist you today?\n",
            "Human: Привіт, мене звати Андрій\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Привіт, Андрій! Радий познайомитися. Як я можу допомогти вам сьогодні?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 315
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Скільки 1+1?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "ZxUQHODzM5WH",
        "outputId": "23a72ab5-13a2-4220-c7c3-0b0884be7126"
      },
      "execution_count": 316,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Andrew\n",
            "AI: Hello Andrew! It's nice to meet you. How can I assist you today?\n",
            "Human: Привіт, мене звати Андрій\n",
            "AI: Привіт, Андрій! Радий познайомитися. Як я можу допомогти вам сьогодні?\n",
            "Human: Скільки 1+1?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1+1 дорівнює 2.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 316
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Як мене звати? Напиши 5-ма різними мовами.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "4Jz6fRfaM5LS",
        "outputId": "da8ef0cb-71e4-44fa-93cd-aaeea75bac29"
      },
      "execution_count": 317,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Andrew\n",
            "AI: Hello Andrew! It's nice to meet you. How can I assist you today?\n",
            "Human: Привіт, мене звати Андрій\n",
            "AI: Привіт, Андрій! Радий познайомитися. Як я можу допомогти вам сьогодні?\n",
            "Human: Скільки 1+1?\n",
            "AI: 1+1 дорівнює 2.\n",
            "Human: Як мене звати? Напиши 5-ма різними мовами.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Ваше ім'я - Андрій. Я можу назвати його на п'яти різних мовах:\\n\\n1. Англійська: Andrew\\n2. Французька: André\\n3. Іспанська: Andrés\\n4. Німецька: Andreas\\n5. Італійська: Andrea\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 317
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zT7eURvjM4um",
        "outputId": "6c58e687-34a1-4f8c-c497-13615d3e83ab"
      },
      "execution_count": 318,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Hi, my name is Andrew\n",
            "AI: Hello Andrew! It's nice to meet you. How can I assist you today?\n",
            "Human: Привіт, мене звати Андрій\n",
            "AI: Привіт, Андрій! Радий познайомитися. Як я можу допомогти вам сьогодні?\n",
            "Human: Скільки 1+1?\n",
            "AI: 1+1 дорівнює 2.\n",
            "Human: Як мене звати? Напиши 5-ма різними мовами.\n",
            "AI: Ваше ім'я - Андрій. Я можу назвати його на п'яти різних мовах:\n",
            "\n",
            "1. Англійська: Andrew\n",
            "2. Французька: André\n",
            "3. Іспанська: Andrés\n",
            "4. Німецька: Andreas\n",
            "5. Італійська: Andrea\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfhSAjvCGE42",
        "outputId": "0041723d-36f9-4b4a-8c51-37bbf46a3451"
      },
      "execution_count": 319,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi, my name is Andrew\\nAI: Hello Andrew! It's nice to meet you. How can I assist you today?\\nHuman: Привіт, мене звати Андрій\\nAI: Привіт, Андрій! Радий познайомитися. Як я можу допомогти вам сьогодні?\\nHuman: Скільки 1+1?\\nAI: 1+1 дорівнює 2.\\nHuman: Як мене звати? Напиши 5-ма різними мовами.\\nAI: Ваше ім'я - Андрій. Я можу назвати його на п'яти різних мовах:\\n\\n1. Англійська: Andrew\\n2. Французька: André\\n3. Іспанська: Andrés\\n4. Німецька: Andreas\\n5. Італійська: Andrea\"}"
            ]
          },
          "metadata": {},
          "execution_count": 319
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ConversationBufferWindowMemory**"
      ],
      "metadata": {
        "id": "ac6wEF9YNTC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory"
      ],
      "metadata": {
        "id": "QtQ6wdzSNCO4"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0.0)\n",
        "memory = ConversationBufferWindowMemory(k=2)\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "0jBQlRSkNCEX"
      },
      "execution_count": 322,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('----------------')\n",
        "print(conversation.predict(input=\"Hi, my name is Andrew\"))\n",
        "print('----------------')\n",
        "print(memory.buffer)\n",
        "print('----------------')\n",
        "print(conversation.predict(input=\"What is 1+1?\"))\n",
        "print('----------------')\n",
        "print(memory.buffer)\n",
        "print('----------------')\n",
        "print(conversation.predict(input=\"What is 103*98?\"))\n",
        "print('----------------')\n",
        "print(memory.buffer)\n",
        "print('----------------')\n",
        "print(conversation.predict(input=\"What is my name?\"))\n",
        "print('----------------')\n",
        "print(memory.buffer)\n",
        "print('----------------')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dJ7Avi7NB7d",
        "outputId": "9015e352-30ae-4755-e613-9848315d687e"
      },
      "execution_count": 323,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------\n",
            "Hello Andrew! It's nice to meet you. How can I assist you today?\n",
            "----------------\n",
            "Human: Hi, my name is Andrew\n",
            "AI: Hello Andrew! It's nice to meet you. How can I assist you today?\n",
            "----------------\n",
            "1+1 is equal to 2.\n",
            "----------------\n",
            "Human: Hi, my name is Andrew\n",
            "AI: Hello Andrew! It's nice to meet you. How can I assist you today?\n",
            "Human: What is 1+1?\n",
            "AI: 1+1 is equal to 2.\n",
            "----------------\n",
            "103 multiplied by 98 equals 10,094.\n",
            "----------------\n",
            "Human: What is 1+1?\n",
            "AI: 1+1 is equal to 2.\n",
            "Human: What is 103*98?\n",
            "AI: 103 multiplied by 98 equals 10,094.\n",
            "----------------\n",
            "I'm sorry, but I don't have access to personal information.\n",
            "----------------\n",
            "Human: What is 103*98?\n",
            "AI: 103 multiplied by 98 equals 10,094.\n",
            "Human: What is my name?\n",
            "AI: I'm sorry, but I don't have access to personal information.\n",
            "----------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ConversationTokenBufferMemory**"
      ],
      "metadata": {
        "id": "cdF4oF0lOoBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationTokenBufferMemory\n",
        "\n",
        "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=30)\n",
        "llm = ChatOpenAI(temperature=0.0)\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "40yeZW_tNBZR"
      },
      "execution_count": 324,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('----------------')\n",
        "print(conversation.predict(input=\"Hi, my name is Andrew\"))\n",
        "print('----------------')\n",
        "print(memory.buffer)\n",
        "print('----------------')\n",
        "print(conversation.predict(input=\"What is 1+1?\"))\n",
        "print('----------------')\n",
        "print(memory.buffer)\n",
        "print('----------------')\n",
        "print(conversation.predict(input=\"What is 103*98?\"))\n",
        "print('----------------')\n",
        "print(memory.buffer)\n",
        "print('----------------')\n",
        "print(conversation.predict(input=\"What is my name?\"))\n",
        "print('----------------')\n",
        "print(memory.buffer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9fc1i_4NBOi",
        "outputId": "2c918c2c-71eb-42f9-cb3f-76583e995232"
      },
      "execution_count": 325,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------\n",
            "Hello Andrew! It's nice to meet you. How can I assist you today?\n",
            "----------------\n",
            "AI: Hello Andrew! It's nice to meet you. How can I assist you today?\n",
            "----------------\n",
            "1+1 is equal to 2.\n",
            "----------------\n",
            "Human: What is 1+1?\n",
            "AI: 1+1 is equal to 2.\n",
            "----------------\n",
            "103 multiplied by 98 equals 10,094.\n",
            "----------------\n",
            "AI: 103 multiplied by 98 equals 10,094.\n",
            "----------------\n",
            "I'm sorry, but I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation. I am designed to respect user privacy and confidentiality.\n",
            "----------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ConversationSummaryBufferMemory**"
      ],
      "metadata": {
        "id": "lj-ZvqvuOq6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationSummaryBufferMemory"
      ],
      "metadata": {
        "id": "c5aa5DrOOmsX"
      },
      "execution_count": 330,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
        "llm = ChatOpenAI(temperature=0.0)\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "tA-0UzNNOmh3"
      },
      "execution_count": 329,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schedule = \"There is a meeting at 8am with your product team. \\\n",
        "You will need your powerpoint presentation prepared. \\\n",
        "9am-12pm have time to work on your LangChain \\\n",
        "project which will go quickly because Langchain is such a powerful tool. \\\n",
        "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
        "from over an hour away to meet you to understand the latest in AI. \\\n",
        "Be sure to bring your laptop to show the latest LLM demo.\"\n",
        "\n",
        "print('----------------')\n",
        "print(conversation.predict(input=f\"Hi, how are you :) I need your help.\"))\n",
        "print('----------------')\n",
        "print(memory.load_memory_variables({}))\n",
        "print('----------------')\n",
        "print(conversation.predict(input=f\"Here is an email: \\n{schedule}\"))\n",
        "print('----------------')\n",
        "print(memory.load_memory_variables({}))\n",
        "print('----------------')\n",
        "print(conversation.predict(input=\"What would be a good demo to show?\"))\n",
        "print('----------------')\n",
        "print(memory.load_memory_variables({}))\n",
        "print('----------------')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wx6GJmCgOmYT",
        "outputId": "8c8dbb33-2b3e-4d89-da03-630e3f669203"
      },
      "execution_count": 331,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------\n",
            "Hello! I'm an AI designed to assist with various tasks. I'm here to help you. What do you need assistance with?\n",
            "----------------\n",
            "{'history': \"Human: Hi, how are you :) I need your help.\\nAI: Hello! I'm an AI designed to assist with various tasks. I'm here to help you. What do you need assistance with?\"}\n",
            "----------------\n",
            "Thank you for sharing the email with me. Based on the information provided, it seems like you have a busy day ahead. You have a meeting with your product team at 8am, so make sure to have your PowerPoint presentation prepared. From 9am to 12pm, you have time to work on your LangChain project, which should go quickly because LangChain is a powerful tool. At noon, you have a lunch appointment at an Italian restaurant with a customer who is driving from over an hour away to meet you and understand the latest in AI. Don't forget to bring your laptop to show the latest LLM demo. Is there anything specific you need help with regarding this schedule?\n",
            "----------------\n",
            "{'history': 'System: The human asks the AI for help with their schedule. The AI reviews an email and summarizes the tasks for the day, including a meeting with the product team, working on the LangChain project, and a lunch appointment with a customer. The AI asks if there is anything specific the human needs help with regarding this schedule.'}\n",
            "----------------\n",
            "It depends on the purpose of the demo and the audience you will be presenting to. Could you provide more information about the context and goals of the demo?\n",
            "----------------\n",
            "{'history': 'System: The human asks the AI for help with their schedule. The AI reviews an email and summarizes the tasks for the day, including a meeting with the product team, working on the LangChain project, and a lunch appointment with a customer. The AI asks if there is anything specific the human needs help with regarding this schedule.\\nHuman: What would be a good demo to show?\\nAI: It depends on the purpose of the demo and the audience you will be presenting to. Could you provide more information about the context and goals of the demo?'}\n",
            "----------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chains**"
      ],
      "metadata": {
        "id": "83_XFSQRQaCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SequentialChain\n",
        "from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "j3DLAcjuOmBu"
      },
      "execution_count": 332,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0.0)\n",
        "\n",
        "# prompt template 1\n",
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    Create a comprehensive step-by-step guide with code snippets for a {specialist_level} user on {topic}.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Chain 1\n",
        "chain_one = LLMChain(llm=llm, prompt=first_prompt, output_key=\"Guide\")\n"
      ],
      "metadata": {
        "id": "MCVJTLaF9zF_"
      },
      "execution_count": 335,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0.0)\n",
        "\n",
        "# prompt template 2\n",
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    Translate the guide into the Ukrainian language.\n",
        "    Guide:\n",
        "    {Guide}\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Chain 2\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt, output_key=\"Guide in UA\")\n"
      ],
      "metadata": {
        "id": "gVnJT5Xw9y8d"
      },
      "execution_count": 334,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_chain = SequentialChain(\n",
        "    chains=[chain_one, chain_two],\n",
        "    input_variables=[\"specialist_level\", \"topic\"],\n",
        "    output_variables=[\"Guide\", \"Guide in UA\"],\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "XKeScJbH9yra"
      },
      "execution_count": 336,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_chain({\"specialist_level\":specialist_level, \"topic\":topic})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8d-Z-iuQ3PO",
        "outputId": "52941987-45f9-4f5e-a1c2-67d72e1ec6b8"
      },
      "execution_count": 337,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'specialist_level': 'Beginner',\n",
              " 'topic': 'how to use openai library',\n",
              " 'Guide': 'Step 1: Install OpenAI Library\\nFirst, you need to install the OpenAI library. Open your terminal or command prompt and run the following command:\\n\\n```\\npip install openai\\n```\\n\\nStep 2: Import the OpenAI Library\\nIn your Python script or notebook, import the OpenAI library using the following code:\\n\\n```python\\nimport openai\\n```\\n\\nStep 3: Set up your OpenAI API key\\nTo use the OpenAI library, you need to set up your API key. You can obtain your API key by signing up on the OpenAI website. Once you have your API key, set it up using the following code:\\n\\n```python\\nopenai.api_key = \\'YOUR_API_KEY\\'\\n```\\n\\nStep 4: Make API calls\\nNow you are ready to make API calls to the OpenAI library. There are several different API endpoints available, depending on the task you want to perform. Here are a few examples:\\n\\n4.1. Text Completion\\nTo generate text completion, use the `openai.Completion.create()` method. Here\\'s an example:\\n\\n```python\\nresponse = openai.Completion.create(\\n  engine=\\'davinci\\',\\n  prompt=\\'Once upon a time\\',\\n  max_tokens=100\\n)\\n\\ncompletion = response.choices[0].text.strip()\\nprint(completion)\\n```\\n\\n4.2. Text Classification\\nTo perform text classification, use the `openai.Classification.create()` method. Here\\'s an example:\\n\\n```python\\nresponse = openai.Classification.create(\\n  model=\\'text-davinci-003\\',\\n  examples=[\\n        [\"A happy moment\", \"Positive\"],\\n        [\"I am sad.\", \"Negative\"],\\n        [\"I am feeling awesome\", \"Positive\"]\\n    ],\\n  query=\\'It is a raining day\\'\\n)\\n\\nlabel = response[\\'label\\']\\nprint(label)\\n```\\n\\n4.3. Text Translation\\nTo perform text translation, use the `openai.Translation.create()` method. Here\\'s an example:\\n\\n```python\\nresponse = openai.Translation.create(\\n  model=\\'text-davinci-003\\',\\n  source_language=\\'en\\',\\n  target_language=\\'fr\\',\\n  text=\\'Hello, how are you?\\'\\n)\\n\\ntranslation = response[\\'translations\\'][0][\\'translation\\']\\nprint(translation)\\n```\\n\\nThese are just a few examples of what you can do with the OpenAI library. There are many more API endpoints available, so make sure to check the OpenAI documentation for more details.\\n\\nStep 5: Handle API responses\\nWhen you make an API call, you will receive a response from the OpenAI API. The response will contain the result of your request. You can access the result using the appropriate attributes or keys, depending on the API endpoint you used. Make sure to handle the response appropriately in your code.\\n\\nThat\\'s it! You now have a comprehensive step-by-step guide on how to use the OpenAI library. Remember to refer to the OpenAI documentation for more details and explore the various API endpoints available.',\n",
              " 'Guide in UA': 'Крок 1: Встановіть бібліотеку OpenAI\\nСпочатку вам потрібно встановити бібліотеку OpenAI. Відкрийте термінал або командний рядок і виконайте наступну команду:\\n\\n```\\npip install openai\\n```\\n\\nКрок 2: Імпортуйте бібліотеку OpenAI\\nУ своєму скрипті Python або ноутбуці імпортуйте бібліотеку OpenAI за допомогою наступного коду:\\n\\n```python\\nimport openai\\n```\\n\\nКрок 3: Налаштуйте свій ключ API OpenAI\\nДля використання бібліотеки OpenAI вам потрібно налаштувати свій ключ API. Ви можете отримати свій ключ API, зареєструвавшись на веб-сайті OpenAI. Після отримання ключа API налаштуйте його за допомогою наступного коду:\\n\\n```python\\nopenai.api_key = \\'ВАШ_КЛЮЧ_API\\'\\n```\\n\\nКрок 4: Виконуйте виклики API\\nТепер ви готові виконувати виклики API до бібліотеки OpenAI. Існує кілька різних кінцевих точок API, залежно від завдання, яке ви хочете виконати. Ось кілька прикладів:\\n\\n4.1. Завершення тексту\\nДля генерації завершення тексту використовуйте метод `openai.Completion.create()`. Ось приклад:\\n\\n```python\\nresponse = openai.Completion.create(\\n  engine=\\'davinci\\',\\n  prompt=\\'Жили-були\\',\\n  max_tokens=100\\n)\\n\\ncompletion = response.choices[0].text.strip()\\nprint(completion)\\n```\\n\\n4.2. Класифікація тексту\\nДля класифікації тексту використовуйте метод `openai.Classification.create()`. Ось приклад:\\n\\n```python\\nresponse = openai.Classification.create(\\n  model=\\'text-davinci-003\\',\\n  examples=[\\n        [\"Щасливий момент\", \"Позитивний\"],\\n        [\"Я сумний.\", \"Негативний\"],\\n        [\"Я почуваюся чудово\", \"Позитивний\"]\\n    ],\\n  query=\\'Це дощовий день\\'\\n)\\n\\nlabel = response[\\'label\\']\\nprint(label)\\n```\\n\\n4.3. Переклад тексту\\nДля перекладу тексту використовуйте метод `openai.Translation.create()`. Ось приклад:\\n\\n```python\\nresponse = openai.Translation.create(\\n  model=\\'text-davinci-003\\',\\n  source_language=\\'en\\',\\n  target_language=\\'fr\\',\\n  text=\\'Привіт, як справи?\\'\\n)\\n\\ntranslation = response[\\'translations\\'][0][\\'translation\\']\\nprint(translation)\\n```\\n\\nЦе лише кілька прикладів того, що ви можете зробити з бібліотекою OpenAI. Існує багато інших кінцевих точок API, тому обов\\'язково перевірте документацію OpenAI для отримання додаткових відомостей.\\n\\nКрок 5: Обробляйте відповіді API\\nПісля виклику API ви отримаєте відповідь від API OpenAI. Відповідь міститиме результат вашого запиту. Ви можете отримати доступ до результату, використовуючи відповідні атрибути або ключі, залежно від використаної кінцевої точки API. Обов\\'язково обробляйте відповідь належним чином у своєму коді.\\n\\nОсь і все! Тепер у вас є докладний посібник з кроками, як використовувати бібліотеку OpenAI. Не забудьте звернутися до документації OpenAI для отримання додаткових відомостей та досліджуйте різні кінцеві точки API, які доступні.'}"
            ]
          },
          "metadata": {},
          "execution_count": 337
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DataBases**"
      ],
      "metadata": {
        "id": "JaXF1bV-UkL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import openai\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.retrievers import TFIDFRetriever\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n"
      ],
      "metadata": {
        "id": "ffeC1RAfUKFP"
      },
      "execution_count": 338,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading PDF File**"
      ],
      "metadata": {
        "id": "hhl4LMYMVn9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "file_paths = [\n",
        "    \"./article.pdf\"\n",
        "]\n",
        "\n",
        "loaders = [\n",
        "    PyPDFLoader(file_path) for file_path in file_paths]\n",
        "\n",
        "docs = []\n",
        "for loader in loaders:\n",
        "    docs.extend(loader.load())\n",
        "\n",
        "pages = loader.load()\n",
        "\n",
        "print(f\"# of pages: {len(pages)}\")\n",
        "print(f\"100 characters: {pages[0].page_content[:100]}\")\n",
        "print(f\"metadata: {pages[0].metadata}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbvlOb0LU7-s",
        "outputId": "e992962d-b3e5-491e-bc3e-a2db4818e459"
      },
      "execution_count": 339,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of pages: 15\n",
            "100 characters: Provided proper attribution is provided, Google hereby grants permission to\n",
            "reproduce the tables and\n",
            "metadata: {'source': './article.pdf', 'page': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Retrieving data from a pdf file**"
      ],
      "metadata": {
        "id": "aPP4hOchVrhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "retriever = TFIDFRetriever.from_documents(pages, k=3)\n",
        "result = retriever.get_relevant_documents(\"Hey, what is the document about. Summarize in several sentences.\")\n",
        "for i in range(len(result)):\n",
        "    result_ = result[i].metadata['page']\n",
        "    print(f\"Page used: {result_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQVcu46cVJFU",
        "outputId": "bdb8b5de-1c7e-4a01-dd71-35cda0e6207e"
      },
      "execution_count": 365,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page used: 14\n",
            "Page used: 13\n",
            "Page used: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result[i]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_ObV08wVP-4",
        "outputId": "7fba452c-d77f-45f9-b3ad-24f6b527036d"
      },
      "execution_count": 343,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='length nis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,\\norO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+n·d2). Even with k=n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [ 20] with β1= 0.9,β2= 0.98andϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate =d−0.5\\nmodel·min(step_num−0.5, step _num·warmup _steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup _steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup _steps = 4000 .\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7', metadata={'source': './article.pdf', 'page': 6})"
            ]
          },
          "metadata": {},
          "execution_count": 343
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conversation with PDF File**"
      ],
      "metadata": {
        "id": "R8905-rGVwxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "qa = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    condense_question_llm = ChatOpenAI(temperature=0, model=llm_name),\n",
        "    verbose=False,\n",
        ")\n"
      ],
      "metadata": {
        "id": "-7RWj1ZeUJii"
      },
      "execution_count": 366,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Summarize the file.\"\n",
        "\n",
        "result = qa({\"question\": question, \"chat_history\":{}})\n",
        "Markdown(result[\"answer\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "NYlOiZRMUvEM",
        "outputId": "7aa6afd1-e6d1-4cf8-eb44-c487a9e1c769"
      },
      "execution_count": 345,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The file discusses the Transformer model architecture, which is used for sequence transduction tasks. The model consists of an encoder and a decoder, each composed of multiple layers. The encoder and decoder layers include self-attention mechanisms and position-wise feed-forward networks. The self-attention mechanism allows the model to attend to different positions in the input sequence, while the feed-forward networks process each position separately. The model also incorporates positional encodings to capture the order of the sequence. The file compares self-attention layers to recurrent and convolutional layers in terms of computational complexity, parallelization, and path length between long-range dependencies. The attention function in the model allows for joint attention across different representation subspaces. The file also discusses the use of embeddings and softmax functions in the model. Overall, the Transformer model offers a powerful and efficient approach for sequence transduction tasks."
          },
          "metadata": {},
          "execution_count": 345
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "qa = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    memory=None,\n",
        "    retriever=retriever,\n",
        "    # condense_question_llm = ChatOpenAI(temperature=0, model=llm_name),\n",
        "    verbose=False,\n",
        ")\n"
      ],
      "metadata": {
        "id": "lgK76GLBYXX0"
      },
      "execution_count": 367,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Find Reference Section\"\n",
        "\n",
        "result = qa({\"question\": question, \"chat_history\":{}})\n",
        "Markdown(result[\"answer\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "0Yo5dtbCUvef",
        "outputId": "45cad591-94f8-479e-dd6c-9781a8bfc008"
      },
      "execution_count": 368,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The reference section is not provided in the given context."
          },
          "metadata": {},
          "execution_count": 368
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.deeplearning.ai/short-courses/\n",
        "https://www.coursera.org/programs/kse-on-coursera-learning-program-ovyi3/browse?query=deep+learning+ai&productTypeDescription=Specializations&source=search"
      ],
      "metadata": {
        "id": "MsL9Sm3xBJgb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Es9zsiaZUvrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfXIOfM29sbU"
      },
      "outputs": [],
      "source": []
    }
  ]
}